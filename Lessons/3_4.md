
## GCS setup
create a yc-retry/data/ folder and put the files in there from the Github data-files folder

## Create BigQuery dataset
go to bigquery and create a yc_retry dataset
It must be in the same location as your GCS bucket! (for me EU)

## Error handling main 

```
id: yellow_cab_retry_main
namespace: gcp.yellowcab

variables:
  bucket: "lde-my-kestra-workspace"
  dataset: "yc_retry"
  project: "kestra-workspace-ak-lde-2025"
  location: "EU"

tasks:
  - id: list_files
    type: io.kestra.plugin.gcp.gcs.List
    from: "gs://{{ vars.bucket }}/yc-retry/data/"
    serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"

  - id: ensure_result_table_exists
    type: io.kestra.plugin.gcp.bigquery.Query
    serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
    projectId: "{{ vars.project }}"
    location: "{{ vars.location }}"
    sql: |
      CREATE TABLE IF NOT EXISTS `{{ vars.project }}.{{ vars.dataset }}.yellowcab_result` (
        VendorID INT64,
        tpep_pickup_datetime TIMESTAMP,
        tpep_dropoff_datetime TIMESTAMP,
        passenger_count INT64,
        trip_distance FLOAT64,
        total_amount FLOAT64,
        source_file STRING,
        ingestion_run_id STRING,
        ingested_at TIMESTAMP
      );

  - id: foreach_file
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{ outputs.list_files.blobs }}"
    tasks:
      - id: process_one_file
        type: io.kestra.plugin.core.flow.Sequential
        runIf: "{{ taskrun.value | jq('.size') | first > 0 }}"
        tasks:

          # ------------------------------------------------------------
          # 1) Load each file into its own raw staging table (no collisions)
          # ------------------------------------------------------------
          - id: raw_load
            type: io.kestra.plugin.gcp.bigquery.LoadFromGcs
            serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
            from:
              - "{{ parent.taskrun.value | jq('.uri') | first }}"
            destinationTable: "{{ vars.project }}.{{ vars.dataset }}.raw_stage_{{ taskrun.parentId }}"
            format: CSV
            autodetect: true
            csvOptions:
              skipLeadingRows: 1
            writeDisposition: WRITE_TRUNCATE
            location: "{{ vars.location }}"
            retry:
              type: exponential
              interval: PT5S
              maxAttempt: 6
              maxInterval: PT1M

          # ------------------------------------------------------------
          # 2) Subflow appends cleaned rows into final table
          # ------------------------------------------------------------
          - id: cleaning_subflow
            type: io.kestra.plugin.core.flow.Subflow
            namespace: gcp.yellowcab
            flowId: yellow_cab_retry_subflow
            wait: true
            inputs:
              file_name: "{{ parent.taskrun.value | jq('.name') | first | split('/') | last }}"
              ingestion_run_id: "{{ execution.id }}"
              raw_table: "{{ vars.project }}.{{ vars.dataset }}.raw_stage_{{ taskrun.parentId }}"

          # ------------------------------------------------------------
          # 3) Drop per-file raw staging table after success
          # ------------------------------------------------------------
          - id: drop_raw_stage_success
            type: io.kestra.plugin.gcp.bigquery.Query
            serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
            projectId: "{{ vars.project }}"
            location: "{{ vars.location }}"
            sql: |
              DROP TABLE IF EXISTS `{{ vars.project }}.{{ vars.dataset }}.raw_stage_{{ taskrun.parentId }}`;
                            
          # ------------------------------------------------------------
          # 4) Archive + delete original after success
          # ------------------------------------------------------------
          - id: copy_to_archive
            type: io.kestra.plugin.gcp.gcs.Copy
            serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
            from: "{{ parent.taskrun.value | jq('.uri') | first }}"
            to: "gs://{{ vars.bucket }}/yc-retry/archive/{{ parent.taskrun.value | jq('.name') | first | split('/') | last }}"

          - id: delete_original_success
            type: io.kestra.plugin.gcp.gcs.Delete
            serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
            uri: "{{ parent.taskrun.value | jq('.uri') | first }}"

        errors:
          # ------------------------------------------------------------
          # A) Roll back rows written for this file + run (realistic failure handling)
          # ------------------------------------------------------------
          - id: rollback_final_rows_for_file
            type: io.kestra.plugin.gcp.bigquery.Query
            serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
            projectId: "{{ vars.project }}"
            location: "{{ vars.location }}"
            sql: |
              DELETE FROM `{{ vars.project }}.{{ vars.dataset }}.yellowcab_result`
              WHERE ingestion_run_id = '{{ execution.id }}'
                AND source_file = '{{ parent.taskrun.value | jq('.name') | first | split('/') | last }}';

          # ------------------------------------------------------------
          # B) Drop per-file raw staging table even on error (100%)
          # ------------------------------------------------------------
          - id: drop_raw_stage_error
            type: io.kestra.plugin.gcp.bigquery.Query
            serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
            projectId: "{{ vars.project }}"
            location: "{{ vars.location }}"
            sql: |
              DROP TABLE IF EXISTS `{{ vars.project }}.{{ vars.dataset }}.raw_stage_{{ taskrun.parentId }}`;

          # Optional debug
          - id: debug_error_ctx
            type: io.kestra.plugin.core.debug.Return
            format: |
              ==== DEBUG ERROR CONTEXT ====
              taskrun: {{ taskrun | json }}
              parent.taskrun: {{ parent.taskrun | json }}

          - id: copy_to_error
            type: io.kestra.plugin.gcp.gcs.Copy
            serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
            from: "{{ parent.taskrun.value | jq('.uri') | first }}"
            to: "gs://{{ vars.bucket }}/yc-retry/errors/{{ parent.taskrun.value | jq('.name') | first | split('/') | last }}"

          - id: delete_original_failed
            type: io.kestra.plugin.gcp.gcs.Delete
            serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
            uri: "{{ parent.taskrun.value | jq('.uri') | first }}"

```
## Error handling subflow

```
id: yellow_cab_retry_subflow
namespace: gcp.yellowcab

variables:
  project: "kestra-workspace-ak-lde-2025"
  dataset: "yc_retry"
  location: "EU"

inputs:
  - id: file_name
    type: STRING
  - id: raw_table
    type: STRING
  - id: ingestion_run_id
    type: STRING

tasks:
    # ---------------------------------------------------------
  # ATTEMPT 1: STRICT CLEANING -> append into final table
  # ---------------------------------------------------------
  - id: insert_strict
    type: io.kestra.plugin.gcp.bigquery.Query
    serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
    projectId: "{{ vars.project }}"
    location: "{{ vars.location }}"
    sql: |
      INSERT INTO `{{ vars.project }}.{{ vars.dataset }}.yellowcab_result`
      SELECT
        CAST(VendorID AS INT64) AS VendorID,
        TIMESTAMP(tpep_pickup_datetime) AS tpep_pickup_datetime,
        TIMESTAMP(tpep_dropoff_datetime) AS tpep_dropoff_datetime,
        SAFE_CAST(passenger_count AS INT64) AS passenger_count,
        SAFE_CAST(trip_distance AS FLOAT64) AS trip_distance,
        SAFE_CAST(total_amount AS FLOAT64) AS total_amount,
        '{{ inputs.file_name }}' AS source_file,
        '{{ inputs.ingestion_run_id }}' AS ingestion_run_id,
        CURRENT_TIMESTAMP() AS ingested_at
      FROM `{{ inputs.raw_table }}`
      WHERE SAFE_CAST(passenger_count AS INT64) IS NOT NULL;

  - id: strict_success
    type: io.kestra.plugin.core.debug.Return
    format: "Strict cleaning insert succeeded for {{ inputs.file_name }}"

errors:
  # ---------------------------------------------------------
  # ATTEMPT 2: SOFT CLEANING -> append into final table
  # ---------------------------------------------------------
  - id: insert_soft
    type: io.kestra.plugin.gcp.bigquery.Query
    serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
    projectId: "{{ vars.project }}"
    location: "{{ vars.location }}"
    sql: |
      INSERT INTO `{{ vars.project }}.{{ vars.dataset }}.yellowcab_result`
      WITH cleaned AS (
        SELECT
          CAST(VendorID AS INT64) AS VendorID,
          TIMESTAMP(tpep_pickup_datetime) AS tpep_pickup_datetime,
          TIMESTAMP(tpep_dropoff_datetime) AS tpep_dropoff_datetime,

          CASE
            WHEN SAFE_CAST(passenger_count AS INT64) IS NULL THEN 1
            ELSE SAFE_CAST(passenger_count AS INT64)
          END AS passenger_count,

          SAFE_CAST(trip_distance AS FLOAT64) AS trip_distance,
          SAFE_CAST(total_amount AS FLOAT64) AS total_amount
        FROM `{{ inputs.raw_table }}`
      )
      SELECT
        VendorID,
        tpep_pickup_datetime,
        tpep_dropoff_datetime,
        passenger_count,
        trip_distance,
        total_amount,
        '{{ inputs.file_name }}' AS source_file,
        '{{ inputs.ingestion_run_id }}' AS ingestion_run_id,
        CURRENT_TIMESTAMP() AS ingested_at
      FROM cleaned
      WHERE passenger_count IS NOT NULL;

  - id: soft_success
    type: io.kestra.plugin.core.debug.Return
    format: "Soft cleaning insert succeeded for {{ inputs.file_name }}"  
      
```

