## Download larger dataset

[NYC Yellow Cab Dataset](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)


Create the BigQuery Datasets
```bash
bq --location=EU mk --dataset kestra-workspace-ak-lde-2025:YellowCab
```


## Flow to cretae the processing - sequentially

```bash
id: yellow_cab_ingest_raw
namespace: gcp.yellowcab

variables:
  bucket: "lde-my-kestra-workspace"

tasks:
  - id: list_files
    type: io.kestra.plugin.gcp.gcs.List
    from: "gs://{{ vars.bucket }}/yellow-cab/"
    serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"

  - id: dump_blobs
    type: io.kestra.plugin.core.debug.Return
    format: |
      OUTPUT BLOBS:
      {{ outputs.list_files.blobs }}

  - id: foreach_file
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{ outputs.list_files.blobs }}"
    tasks:
      - id: load
        runIf: "{{ taskrun.value | jq('.size') | first > 0 }}"
        type: io.kestra.plugin.gcp.bigquery.LoadFromGcs
        serviceAccount: "{{ secret('GCP_SERVICE_ACCOUNT') }}"
        from:
          - "{{ taskrun.value | jq('.uri') | first }}"
        destinationTable: "kestra-workspace-ak-lde-2025.YellowCab.raw_data"
        format: PARQUET
        location: "EU"
        writeDisposition: WRITE_APPEND
```

## Add paralell processing to GCS load
Add this to your ForEach task:

```
concurrencyLimit: 5
```